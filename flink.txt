官网
https://ci.apache.org/projects/flink/flink-docs-stable/

教程
https://my.oschina.net/duanvincent/blog

视频教程
https://blog.csdn.net/xueyao0201/article/details/100065804

flink+Spring依赖配置
https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/dependencies.html

DataX和Kettle速度对比
https://www.cnblogs.com/tianyafu/p/9907391.html


maven中pom.xml文件经常用到dependency依赖，用于引入jar文件
<groupId></groupId>         表明你要引入的jar是哪个组的
<artifaceId></artifaceId>   表明在这个组当中的唯一性
<version></version>         表明jar的版本
<scope></scope>             表明jar文件的作用范围

<scope>值的作用范围：
1. compile 默认值，适用于所有阶段（表明该jar包在编译、运行以及测试中均可见），并且会随着项目直接发布。
2. provided 编译和测试时有效，运行时由服务器提供。如servlet-api
3. runtime 运行时适用，对测试和运行有效。如jdbc
4. test 只在测试时使用，在编译和运行时不起作用。
5. system 不依赖maven仓库解析，需要提供依赖的显式jar包路径


在IDEA等开发工具中运行代码的时候，需要把依赖配置中的scope属性注释掉。在编译打JAR包的时候，需要开启scope属性，这样最终的JAR包就不会把这些依赖包也包含进取，因为集群中本身是有Flink的相关依赖的


大数据计算引擎发展经历了几个过程，从第1代的MapReduce，到第2代基于有向无环图的Tez，第3代基于内存计算的Spark，再到第4代的Flink
Flink可以基于Hadoop进行开发和使用，所以Flink并不会取代Hadoop，而是和Hadoop紧密结合


Flink是一个开源的流处理框架，其特点如下

1.分布式：Flink程序可以运行在多台机器上
2.性能：处理性能比较高
3.高可用：由于Flink程序本身是稳定的，因此它支持高可用性（High Availability，HA）
4.准确: Flink可以保证数据处理的准确性

Flink主要由Java代码实现，它同时支持实时流处理和批处理，对于Flink批数据只是流数据的一个极限特例而已。此外，Flink还支持迭代计算、内存管理和程序优化

1.流式优先：Flink可以连续处理流式数据
2.容错：Flink提供有状态的计算，可以记录数据的处理状态，当数据处理失败的时候，能无缝地从失败中恢复，并保持Exactly-one
3.可伸缩：Flink中的一个集群支持上千个节点
4.性能：Flink支持高吞吐、低延迟

高吞吐表示单位时间内可以处理的数据量很大，低延迟表示数据产生以后可以在很短的时间内对其进行处理，也就是Flink可以支持快速地处理海量数据。


Flink的架构可以分为4层，包括Deploy、Core层、API层、Library层
1.Deploy层：该层主要涉及Flink的部署模式，Flink支持多种部署模式——本地、集群（Standalone/YARN）和云服务器（GCE/EC2）
2.Core层：该层提供了支持Flink计算的全部核心实现，为API层提供基础服务
3.API层：该层主要实现了面向无界Stream的流处理和面向Batch的批处理API，其中流处理对应DataStream API，批处理对应DataSet API
4.Library层：该层也被称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持FlinkML（机器学习库）、Gelly（图处理）、Table操作


Hadoop MapReduce = Map + Reduce
Storm Topology = Spout + Bolt
Flink Job = DataSource + Transformation + DataSink
Flink DataStream API 主要分为3块：DataSource、Transformation、Sink
1.DataSource：表示数据源组件，主要用来接收数据，例如readTextFile、socketTextStream、fromCollection、addSource以及一些第三方Source
2.Transformation：表示算子，主要用来对一个或者多个输入数据源进行计算处理，比如 Map、FlatMap、Filter、Reduce、Aggregation 等
3.DataSink：表示输出组件，主要用来把计算的结果输出到其他存储介质中，比如 writeAsText 以及 Kafka、Redis、Elasticsearch 等等第三方 Sink 组件


Storm 只支持流处理任务，而 MapReduce、Spark 只支持批处理任务
Spark Streaming 本质上海是基于 Spark 批处理系统，采用了一种 Micro-Batch 架构，即把输入的数据流切分成细粒度的 Batch，并为每个 Batch 数据提交一个批处理的 Spark 任务


在执行引擎层级，流处理系统与批处理系统最大的不同在于节点间的数据传输方式
对于一个流处理系统，其节点间数据传输的标准模型是，在处理完成一条数据后，将其序列化到缓存中，并立刻通过网络传输到下一个节点，由下一个节点继续处理。
对于一个批处理系统，其节点间数据传输的标准模型是，在处理完成一条数据后，将其序列化到缓存中，当缓存写满时，就持久化到本地硬盘上，在所有数据都被处理完成后，才开始将其通过网络传输到下一个节点
这两种数据传输模式是两个极端，对应的是流处理系统对低延迟和批处理系统对高吞吐的要求。
Flink通过调整缓存块的超时阈值，用户可以根据需求灵活地权衡系统延迟和吞吐量


流式计算框架对比
产品               模型                           保证次数         容错机制           状态管理                    延时     吞吐量
Storm             Native（数据进入立即处理）          At-least-once  Record ACK        无                         低       低
Trident           Micro-Batching（划分为小批处理）   Exactly-once   Record ACK        基于操作（每次操作有一个状态）  中       中
Spark Streaming   Micro-Batching                 Exactly-once   RDD CheckPoint    基于DStream                中       高
Flink             Native                         Exactly-once   CheckPoint(快照)   基于操作                    低       高


Flink主要应用于流式数据分析场景，包含如下领域
1.实时ETL：集成流计算现有的诸多数据通道和SQL灵活的加工能力，对流式数据进行实时清洗、归并和结构化处理；同时，对离线数仓进行有效的补充和优化，并为数据实时传输提供可计算通道
2.实时报表：实时化采集、加工流式数据存储；实时监控和展现业务、客户各类指标，让数据化运营实时化
3.监控预警：对系统和用户行为进行实时检测和分析，以便及时发现危险行为
4.在线系统：实时计算各类数据指标，并利用实时结果及时调整在线系统的相关策略，在各类内容投放、无线智能推送领域有大量的应用

Flink在如下类型的公司中有具体的应用
1.优化电商网站实时搜索结果：          阿里巴巴的基础设施团队使用Flink实时更新产品细节和库存信息（Blink）
2.针对数据分析团队提供实时流处理服务：通过Flink数据分析平台提供实时数据分析服务，及时发现问题
3.网络/传感器检测和错误检测：         Bouygues电信公司时法国著名的电信供应商，使用Flink监控其有线和无线网络，实现快速故障响应
4.商业智能分析ETL：                 Zalando使用Flink转换数据以便于将其加载到数据仓库，简化复杂的转换操作，并确保分析终端用户可以更快地访问数据（实时ETL）




Flink程序是延迟计算的，只有最后调用execute方法的时候，才会真正触发执行程序
batch的print()中已经加入了execute。注意，是batch不是stream
executeSql()里面已经加入了 execute
所以上述两种情况下，代码使用者不需要再在代码末尾加入execute()
延迟计算的好处：你可以开发复杂的程序，Flink会将这个复杂的程序转为一个Plan，并将Plan作为一个整体单元执行




Apache Flink是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算


第1章  Flink概述
第2章  Flink快速入门
第3章  Flink的安装和部署
第4章  Flink常用API详解

Flink中提供了4种不同层次的API，每种API在简洁和易用之间有自己的权衡
1.低级 API（Stateful Stream Processing）：提供了对时间和状态的细粒度控制，简洁性和易用性差，主要应用在对一些复杂事件的处理逻辑上
2.核心 API（DataStream/DataSet API）：主要提供了针对流数据和离线数据的处理，对低级API进行了一些封装，提供了filter、sum、max、min等高级函数，简单且易用，所以在工作中应用比较广泛
3.Table API（声明式DSL语法）：一般与DataSet或者DataStream紧密关联，首先通过一个DataSet或DataStream创建出一个Table；然后用类似于filter、join或者select关系型转化操作来转化为一个新的Table对象；最后将一个Table对象转回一个DataSet或DataStream。与SQL不同的是，Table API的查询不是一个指定的SQL字符串，而是调用指定的API方法
4.SQL（高级语言）：Flink的SQL集成是基于Apache Calcite的，Apache Calcite实现了标准的SQL，用起来比其他API更加灵活，因为可以直接使用SQL语句。Table API和SQL可以很容易地结合在一块使用，它们都返回Table对象

【DataStream】
一. DataSource
1.基于文件: readTextFile 读取文本文件，文件遵循TextInputFormat逐行读取规则并返回
2.基于Socket: socketTextStream 从Socket中读取数据，元素可以通过一个分隔符分开
3.基于集合: fromCollection 通过Java的Collection集合创建一个数据流，集合中的所有元素必须是相同类型的
4.自定义输入: addSource可以实现读取第三方数据源的数据

内置连接器
连接器                          是否提供Source支持         是否提供Sink支持
Apache Kafka                   Y                        Y
Apache Cassandra               N                        Y
Amazon Kinesis Data Streams    Y                        Y
Elasticsearch                  N                        Y
Hadoop FileSystem              N                        Y
RabbitMQ                       Y                        Y
Apache NiFi                    Y                        Y
Twitter Streaming API          Y                        N
    
Apache Bahir组件支持的连接器信息
连接器                          是否提供Source支持         是否提供Sink支持
Apache ActiveMQ                Y                        Y
Apache Flume                   N                        Y
Redis                          N                        Y
Akka                           N                        Y
Netty                          Y                        N

DataSource提供的容错情况
DataSource          语义保证
File                Exactly-once（仅一次）
Collection          Exactly-once（仅一次）
Socket              At-most-once（最多一次）
Kafka               Exactly-once（仅一次）

也可以自定义数据源，有两种方式实现
1.通过实现SourceFunction接口来自定义无并行度（也就是并行度只能为1）的数据源
2.通过实现ParallelSourceFunction(run,cancel)接口或者继承RichParallelSourceFunction(run,cancel,open,close)来自定义有并行度的数据源

二. Transformation
1.Map: 输入一个元素，然后返回一个元素，中间可以进行清洗转换等操作
2.FlatMap: 输入一个元素，可以返回零个、一个或者多个元素
3.Filter: 过滤函数，对传入的数据进行判断，符合条件的数据会被留下
4.KeyBy: 根据指定的Key进行分组，Key相同的数据会进入同一个分区
 KeyBy的两种典型用法如下：
 （1）DataStream.keyBy("someKey") 指定对象中的someKey字段作为分组key
 （2）DataStream.keyBy(0) 指定Tuple中的第一个元素作为分组key
5.Reduce: 对数据进行聚合操作，结合当前元素和上一次Reduce返回的值进行聚合操作，然后返回一个新的值
6.Aggregations: sum(), min(), max()等
7.Union: 合并多个流，新的流会包含所有流中的数据，但是Union有一个限制，就是所有合并的流类型必须是一致的
8.Connect: 和Union类似，但是只能连接两个流，两个流的数据类型可以不同，会对两个流中的数据应用不同的处理方法
9.coMap, coFlatMap: 在ConnectedStream中需要使用这种函数，类似于Map和flatMap
10.Split: 根据规则把一个数据流切分为多个流
11.Select:和Split配合使用，选择切分后的流

另外，Flink针对DataStream提供了一些数据分区规则
1.Random partitioning: 随机分区。DataStream.shuffle()
2.Rebalancing: 对数据集进行再平衡、重分区和消除数据倾斜。DataStream.rebalance()
3.Rescaling: 重新调节。DataStream.rescale()。Rescaling与Rebalancing的区别是，Rebalance会产生全量重分区，而Rescaling不会
4.Custom partitioning: 自定义分区
 自定义分区实现Partitioner接口的方法如下
 （1）DataStream.partitionCustom(partitioner, "someKey")
 （2）DataStream.partitionCustom(partitioner, 0)
 
三. Sink
1.writeAsText: 将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取
2.print/printToErr: 打印每个元素的toString()方法的值到标准输出或者标准错误输出流中
3.自定义输出: addSink可以实现把数据输出到第三方存储介质中

系统提供了一批内置的Connector，它们会提供对应的Sink支持
Flink通过Apache Bahir组件也提供了对这些Connector的支持
针对这些Sink组件，它们可以提供的容错性保证如下
Sink            语义保证
HDFS            Exactly-once   
Elasticsearch   At-least-once
Kafka Produce   Exactly-once(kafka 0.11)/At-least-once(kafka 0.10)
File            At-least-once
Redis           At-least-once

自定义Sink，有两种实现方案
1.实现SinkFunciton接口
2.继承RichSinkFunction类

【DataSet】
1.DataSource：是程序的数据源输入
2.Transformation：是具体的操作，它对一个或多个输入数据源进行计算处理，比如Map、FlatMap、Filter
3.Sink：是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中

一、DataSource
对于DataSet批处理而言，较频繁的操作是读取HDFS中的文件数据
1.基于集合：fromCollection(Collection)，主要用于测试
2.基于文件：readTextFile(path)，基于HDFS中的数据进行计算分析

二、Transformation
1.Map：输入一个元素，然后返回一个元素，中间可以进行清洗转换等操作
2.FlatMap：输入一个元素，可以返回零个、一个或多个元素
3.MapPartition：类似Map，一次处理一个分区的数据（如果在进行Map处理的时候需要获取第三方资源链接，建议使用）
4.Filter：过滤函数，对传入的数据进行判断，复合条件的数据会被留下
5.Reduce：对数据进行聚合操作，结合当前元素和上一次Reduce返回的值进行聚合操作，然后返回一个新的值
6.Aggregations：sum、max、min
7.Distinct：返回一个数据集中去重之后的元素
8.Join：内连接
9.OuterJoin：外连接
10.Cross：获取两个数据集的笛卡尔积
11.Union：返回两个数据集的总和
12.First-n：获取集合中的前N个元素
13.Sort Partition：在本地对数据集的所有分区进行排序，通过sortPartition()的链接调用来完成对多个字段的排序
Flink针对DataSet提供了一些数据分区规则
（1）Rebalance：对数据集进行再平衡、重分区以及消除数据倾斜操作
（2）Hash-Partition：根据指定Key的散列值对数据集进行分区
（3）Range-Partition：根据指定的Key对数据集进行范围分区
（3）Custom Partition：自定义分区规则，自定义分区需要实现Partitioner接口，例如
partitionCustom(partitioner, "someKey"), partitionCustom(partition, 0)

三、Sink
1.writeAsText：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取
2.writeAsCsv：将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置。每个字段的值来自对象的toString()方法
3.print：打印每个元素的toString()方法的值到标准输出或者标准错误输出流中

Flink Table API和SQL的分析及使用
Flink针对标准的流处理和批处理提供了两种关系型API：Table API和SQL。针对批处理和流处理可以提供相同的处理语义和结果。
1.Table API允许用户以一种很直观的方式进行select、filter和join操作；
2.Flink SQL支持基于Apache Calcite实现的标准SQL
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table_2.12</artifactId>
  <version>1.7.2</version> //新版的依赖maven上没有对应jar包
</dependency>

TableEnvironment对象是Table API和SQL集成的核心，通过TableEnvironment可以实现以下功能
1.通过内部目录创建表
2.通过外部目录创建表
3.执行SQL查询
4.注册一个用户自定义的Function
5.把DataStream或者DataSet转换成Table
6.持有ExecutionEnvironment或者StreamExecutionEnvironment的引用

一个查询中只能绑定一个指定的TableEnvironment，TableEnvironment可以通过TableEnvironment.getTableEnvironment()或者TableConfig来生成。TableConfig可以用来配置TableEnvironment或者自定义查询优化
//流数据查询
StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment sTableEnv = TableEnvironment.getTableEnvironment(sEnv);
//批数据查询
ExecutionEnvironment bEnv = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment bTableEnv = TableEnvironment.getTableEnvironment(bEnv);

通过获取到的TableEnvironment对象可以创建Table对象，有两种类型的Table对象：输入Table(Input Table)和输出Table(Output Table)。输入Table可以给Table API和SQL提供查询数据，输出Table可以把Table API和SQL的查询结果发送到外部存储介质中。

输入Table可以通过多种数据源注册
1.已存在的Table对象：通常是Table API和SQL的查询结果
2.TableSource：通过它可以访问外部数据，比如文件、数据库和消息队列
3.DataStream或DataSet
输出Table需要使用TableSink注册

第5章  Flink高级功能的使用
第6章  Flink State管理与恢复
第7章  Flink窗口详解
第8章  Flink Time详解
第9章  Flink并行度详解
第10章 Flink Kafka Connector详解

第11章 Flink实战项目开发
1.实时数据清洗（实时ETL）
假设目前公司中有四五百台前端业务机器，每天产生T级别的业务日志数据。由于业务原因，我们把几十种类型的日志数据通过一个接口进行日志记录。这些日志数据中的个别字段需要进行转换[如国家（地区）和大区之间的关系，一个大区对应多个国家（地区）码，因为大区和国家（地区）的对应关系会变动，所以日志中存储的是国家（地区）码，在具体使用的时候需要转换]，并且最好根据类型分开统计这些日志数据，这样可以提高后面计算程序的效率，因此需要对源日志数据进行转换，并且根据数据类型分开存储数据。
架构：应用日志——>Kafka集群——>Flink+Redis做抽取转换——>Kafka集群——>Flume做拆分——>HDFS/Kafka
架构分析：
1.使用Flume采集前端业务机器（应用服务器）上的日志数据，使用Exec Source监控指定文件日志数据的产生。在这里注意，需要使用tail -F，而不是tail -f，否则会导致文件重命名后无法采集新文件中的数据
2.通过Flume把机器中的日志数据采集到Kafka中的一个Topic中，Topic的名称是allData
3.通过Flink读取Kafka中的allData进行实时转换。首先需要对数据进行拆分，因为原始日志数据是一个嵌套JSON，我们需要把嵌套JSON进行拆分。然后再从Redis中获取最新的大区和国家（地区）码之间的映射关系，在日志中增加大区字段。在这里需要用Flink中的Connect操作把原始日志数据和Redis中的大区国家（地区）码映射关系数据关联到一起
4.数据解析完成后，通过FlinkKafkaProducer011把数据写到Kafka中的allDataClean中
5.这时所有类型的数据都存储到Kafka中的allDataClean了，为了减轻之后实时计算程序的压力，最好把数据拆分开，不同类型的数据存储到不同的Topic中。在日志中有一个Type字段，根据这个字段的值可以把数据分开存储。Flume中使用RegexExtractorInterceptor拦截器来实现这个功能，提取Type字段，可以在指定Sink Topic的时候使用这个提取的变量字段。这部分数据主要是为了给后面的其它实时计算程序提供数据
6.为了后期可以对数据进行离线计算，在这里通过Flume对数据进行分类落盘操作，使用RegexExtractorInteceptor拦截器提取Type字段，把不同类型的数据存储到HDFS的不同目录下
2.实时数据报表
实时数据报表针对一些需要实时统计的业务指标进行统计。在这里以直播平台和短视频平台中的审核指标进行统计，针对审核结果，有过审（上架）、未过审（下架）、拉黑、推荐、上首页等类型。我们希望能够通过图表实时展现这些审核指标。数据统计的间隔满足分钟级别即可，当然精确到秒级别也是可以的，具体需要看是否能够满足业务需求
架构：应用日志——>Kafka集群——>Flink对数据进行抽取转换——>ElasticSearch+Kibana
架构分析：
1.使用Flume采集前端业务机器（应用服务器）上的日志数据，使用Exec Source监控指定文件日志数据的产生。这里注意，需要使用tail -F，不能使用tail -f，否则会导致文件重命名后无法采集新文件中的数据
2.通过Flume把机器中的日志数据采集到Kafka的一个Topic中，Topic的名称是auditLog
3.通过Flink读取Kafka中的auditLog。首先进行过滤操作，把异常数据过滤掉。然后使用Watermark解决数据乱序的问题。因为在这里要进行分钟级别的汇总统计，所有如果想保证数据的准确性，就要处理数据乱序的问题。可以使用allowedLateness设置一个最大允许乱序时间，再使用sideOutputLateData把迟到太久的数据收集起来，方便后期排查问题。
4.再通过ElasticsearchSink把Flink最终计算的数据结果保存到ES中。
5.最后在ES后添加一个Kibana，可以很方便地查询ES中的数据并创建一些图表



安装
wget --no-check-certificate https://dlcdn.apache.org/flink/flink-1.14.3/flink-1.14.3-bin-scala_2.12.tgz
由于证书过期，需要用--no-check-certificate命令



flink的开放端口在flink-conf.yaml
taskmanager.rpc.port: 50100-50200
metrics.internal.query-service.port: 50201-50300
blob.server.port: 50301-50400
还有默认的8081、6123


并发Concurrency
将相互独立的执行过程综合到一起
并行Paralelism
同时执行(通常是相关的)计算任务的编程技术

并发 vs. 并行 的区别
并发是指同时处理很多事情
而并行是指同时能完成很多事情
一个重点是组合，一个重点是执行